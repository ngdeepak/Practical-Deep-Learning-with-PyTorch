{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701c880-32ac-4969-8a3a-63096e33432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "sentence_model = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "tokenizer = AutoTokenizer.from_Pre-trained(sentence_model)\n",
    "\n",
    "# Derive mean pooling for sentence model\n",
    "def mean_pooling(model_input, attention_mask_input):\n",
    "   token_embeddings = model_input[0] #First element of model_input contains all token embeddings\n",
    "   input_mask_expanded = attention_mask_input.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "   sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "   sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "   return sum_embeddings / sum_mask\n",
    "\n",
    "class SentenceBertModelClass(torch.nn.Module):\n",
    "   def __init__(self, model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\", in_features=384):\n",
    "       super(SentenceBertModelClass, self).__init__()\n",
    "       self.l1 = AutoModel.from_Pre-trained(model_name)\n",
    "       self.pre_classifier = torch.nn.Linear(in_features*3, 768)\n",
    "       self.dropout = torch.nn.Dropout(0.3)\n",
    "       self.classifier = torch.nn.Linear(768, 1)\n",
    "       self.classifierSigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "   def forward(self, sent_ids_var, doc_ids_var, sent_mask_var, doc_mask):\n",
    "       sent_output = self.l1(input_ids=sent_ids_var, attention_mask_input=sent_mask_var)\n",
    "       sentence_embeddings_var = mean_pooling(sent_output, sent_mask_var)\n",
    "       doc_output = self.l1(input_ids=doc_ids_var, attention_mask_input=doc_mask)\n",
    "       doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
    "       # elementwise product of sentence embs and doc embs\n",
    "       combined_features = sentence_embeddings_var * doc_embeddings\n",
    "       # Concatenate input features and their elementwise product\n",
    "       concat_features = torch.cat((sentence_embeddings_var, doc_embeddings, combined_features), dim=1)\n",
    "       pooler_var = self.pre_classifier(concat_features)\n",
    "       pooler_var = torch.nn.ReLU()(pooler_var)\n",
    "       pooler_var = self.dropout(pooler_var)\n",
    "       output_var = self.classifier(pooler_var)\n",
    "       output_var = self.classifierSigmoid(output_var)\n",
    "       return output_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd54866-00f1-40f0-84cc-e34ce9c293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model_var = SentenceBertModelClass(model_name=sentence_model)\n",
    "model_var.to(device)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752ee89-58e2-431b-848b-5167e016b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "   tr_loss = 0\n",
    "   n_correct = 0\n",
    "   nb_tr_steps = 0\n",
    "   nb_tr_examples = 0\n",
    "   model_var.train()\n",
    "   for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "       sent_ids_var = data['sent_ids'].to(device, dtype = torch.long)\n",
    "       doc_ids_var = data['doc_ids'].to(device, dtype = torch.long)\n",
    "       sent_mask_var = data['sent_mask'].to(device, dtype = torch.long)\n",
    "       doc_mask_var = data['doc_mask'].to(device, dtype = torch.long)\n",
    "       targets = data['targets'].to(device, dtype = torch.float)\n",
    "       outputs_var = model_var(sent_ids_var, doc_ids_var, sent_mask_var,\n",
    "       doc_mask_var)\n",
    "       loss_var = loss_function(outputs_var, targets)\n",
    "       tr_loss += loss_var.item()\n",
    "       n_correct += torch.count_nonzero(targets == (outputs_var > 0.5)).item()\n",
    "       nb_tr_steps += 1\n",
    "       nb_tr_examples+=targets.size(0)\n",
    "       if _%print_n_steps==0:\n",
    "           loss_step = tr_loss/nb_tr_steps\n",
    "           accu_step = (n_correct*100)/nb_tr_examples\n",
    "           print(str(_* train_params[\"batch_size\"]) + \"/\" +\n",
    "           str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\",\n",
    "           loss_step)\n",
    "           acc_step_holder.append(accu_step), loss_step_holder.append(loss_step)\n",
    "       optimizer.zero_grad()\n",
    "       loss_var.backward()\n",
    "       # # When using GPU\n",
    "       optimizer.step()\n",
    "\n",
    "   print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "   epoch_loss = tr_loss/nb_tr_steps\n",
    "   epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "   print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "   print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "   return\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "   train(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5f6a1-b9a4-44f3-8375-e0220ad47761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, testing_loader):\n",
    "   model.eval()\n",
    "   n_correct = 0; n_wrong = 0; total = 0;  tr_loss = 0; nb_tr_steps = 0 ; nb_tr_examples = 0\n",
    "   with torch.no_grad():\n",
    "       for _, data in enumerate(testing_loader, 0):\n",
    "           sent_ids_var = data['sent_ids'].to(device, dtype = torch.long)\n",
    "           doc_ids_var = data['doc_ids'].to(device, dtype = torch.long)\n",
    "           sent_mask_var = data['sent_mask'].to(device, dtype = torch.long)\n",
    "           doc_mask_var = data['doc_mask'].to(device, dtype = torch.long)\n",
    "           targets = data['targets'].to(device, dtype = torch.float)\n",
    "           outputs = model(sent_ids_var,doc_ids_var, sent_mask_var,doc_mask \n",
    "           _var)\n",
    "           loss_var = loss_function(outputs, targets)\n",
    "           tr_loss += loss_var.item()\n",
    "           n_correct += torch.count_nonzero(targets == (outputs >\n",
    "           0.5)).item()\n",
    "           nb_tr_steps += 1\n",
    "           nb_tr_examples+=targets.size(0)\n",
    "           if _%print_n_steps==0:\n",
    "               loss_step = tr_loss/nb_tr_steps\n",
    "               accu_step = (n_correct*100)/nb_tr_examples\n",
    "               print(str(_* test_params[\"batch_size\"]) + \"/\" +\n",
    "   str(len(train_df)) + \" - Steps. Acc ->\", accu_step, \"Loss ->\", loss_step)\n",
    "   epoch_loss = tr_loss/nb_tr_steps\n",
    "   epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "   print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "   print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "   return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08c200-34a5-4276-b02e-e3f6e859dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# tokenize text as required by BERT-based models\n",
    "def get_tokens(text, tokenizer):\n",
    " inputs = tokenizer.batch_encode_plus(\n",
    "           text,\n",
    "           add_special_tokens=True,\n",
    "           max_length=512,\n",
    "           padding=\"max_length\",\n",
    "           return_token_type_ids=True,\n",
    "           truncation=True\n",
    "       )\n",
    " ids = inputs['input_ids']\n",
    " mask = inputs['attention_mask_input']\n",
    " return ids, mask\n",
    "\n",
    "# get predictions given some an array of sentences and their corresponding documents\n",
    "def predict(sents, doc):\n",
    " sent_id_var, sent_mask_var = get_tokens(sents,tokenizer)\n",
    " sent_id_var, sent_mask_var = torch.tensor(sent_id_var,\n",
    " dtype=torch.long),torch.tensor(sent_mask_var, dtype=torch.long)\n",
    " doc_id_var, doc_mask = get_tokens([doc],tokenizer)\n",
    " doc_id_var, doc_mask = doc_id_var * len(sents), doc_mask* len(sents)\n",
    " doc_id_var, doc_mask = torch.tensor(doc_id_var,\n",
    " dtype=torch.long),torch.tensor(doc_mask, dtype=torch.long)\n",
    " preds = model(sent_id_var, doc_id_var, sent_mask_var, doc_mask)\n",
    " return preds\n",
    "\n",
    "def summarize(doc, model, min_sentence_length=14, top_k=4, batch_size=3):\n",
    " doc = doc.replace(\"\\n\",\"\")\n",
    " doc_sentences = []\n",
    " for sent in nlp(doc).sents:\n",
    "   if len(sent) > min_sentence_length:\n",
    "     doc_sentences.append(str(sent))\n",
    " doc_id_var, doc_mask = get_tokens([doc],tokenizer)\n",
    " doc_id_var, doc_mask = doc_id_var * batch_size, doc_mask* batch_size\n",
    " doc_id_var, doc_mask = torch.tensor(doc_id_var, dtype=torch.long),torch.tensor \n",
    " (doc_mask, dtype=torch.long)\n",
    " scores_var = []\n",
    " # run predictions using some batch size\n",
    " for i in tqdm(range(int(len(doc_sentences) / batch_size) + 1)):\n",
    "   preds_var = predict(doc_sentences[i*batch_size: (i+1) * batch_size], doc)\n",
    "   scores_var = scores_var + preds_var.tolist()\n",
    " sent_pred_list = [{\"sentence\": doc_sentences[i], \"score\": scores_var[i][0],\n",
    " \"index\":i} for i in range(len(doc_sentences))]\n",
    " sorted_sentences = sorted(sent_pred_list, key=lambda k: k['score'],\n",
    " reverse=True)\n",
    " sorted_result = sorted_sentences[:top_k]\n",
    " sorted_result = sorted(sorted_result, key=lambda k: k['index'])\n",
    " summary = [ x[\"sentence\"] for x in sorted_result]\n",
    " summary = \" \".join(summary)\n",
    " return summary, scores_var, doc_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a9dfe-3c0d-4518-a15f-5e6a29aaeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_Pre-trained(model_name)\n",
    "model = BertForQuestionAnswering.from_Pre-trained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cf0f8-1d5b-4855-91bd-b45a017c7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    start_logits, end_logits = model(**inputs).start_logits, model(**inputs).end_logits\n",
    "    start_idx = start_logits.argmax().item()\n",
    "    end_idx = end_logits.argmax().item()\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_idx : end_idx + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"Mount Everest is the highest peak in the world.\"\n",
    "question = \"What is the height of Mount Everest?\"\n",
    "predicted_answer = predict_answer(context, question)\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f23118-3155-44f7-9cd3-65ed84d03c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
