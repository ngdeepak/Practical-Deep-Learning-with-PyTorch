{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ba3a1-f47d-4985-be8a-0a8f594394ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "sentence = \"You can now install TorchText using pip!\"\n",
    "tokens = tokenizer(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4115a-2f41-4832-9e2c-6983e9bcbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.models import T5Transform\n",
    "padding_idx = 0\n",
    "eos_idx = 1\n",
    "max_seq_len = 512\n",
    "t5_sp_model_path = \"[^1^][1]\"\n",
    "transform = T5Transform(\n",
    "    sp_model_path=t5_sp_model_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    eos_idx=eos_idx,\n",
    "    padding_idx=padding_idx,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a8bf8-e984-4cb5-9bc4-29eb39059bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "tokens = word_tokenize(example_sent)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "print(filtered_tokens)\n",
    "# Output: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886533-e746-496b-bda2-52bab9af0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens)\n",
    "# Output: ['sample', 'sentence', ',', 'showing', 'stop', 'word', 'filtration', '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea324691-8600-4b47-a925-dd804c794863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "# Output: ['sampl', 'sentenc', ',', 'show', 'stop', 'word', 'filtrat', '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a15e4-a7c7-4b99-af4b-d9b5aca72255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9433d-fd7c-4ba1-b71a-d40b3a7eeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Get datasets\n",
    "text_field = data.Field(lower=True, batch_first=True, tokenize='spacy')\n",
    "label_field = data.Field(sequential=False, unk_token = None)\n",
    "train, test = datasets.TREC.splits(text_field, label_field)\n",
    "print('Train length:',str(len(train)))\n",
    "print('Test length:',str(len(test)))\n",
    "# Show some examples\n",
    "for i in range(10):\n",
    "    random_index = random.randint(0,len(train))\n",
    "    print(' '.join(train.examples[random_index].text), train.examples[random_index].label)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_dim, rnn_dim, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_tokens, embedding_dim)\n",
    "        self.rnn = nn.LSTM(input_size = embedding_dim,\n",
    "                           hidden_size = rnn_dim, \n",
    "                           num_layers = num_layers,\n",
    "                           batch_first = True)\n",
    "        self.linear = nn.Linear(rnn_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embeddings(x)\n",
    "        rnn_output, rnn_hidden = self.rnn(emb)\n",
    "        output = self.linear(rnn_output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "n_hidden = 128\n",
    "model = RNN(word_count, embedding_dim=128, rnn_dim=128, num_layers=1, num_classes=classes_count)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76528f87-9b38-41c3-8535-e7d8b712112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_decay = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99) \n",
    "# Scheduled learnint rate, which decays the learning rate exponentially. This could potentially help arrive to a lower minimum.\n",
    "# Another option could be:\n",
    "#lr_decay = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, factor=0.95, patience=10 )\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "train_iter = data.BucketIterator(train, \n",
    "                                 batch_size=batch_size, \n",
    "                                 sort_within_batch=True, \n",
    "                                 shuffle = True, \n",
    "                                 repeat = False)\n",
    "test_iter = data.BucketIterator(test, \n",
    "                          batch_size=30, \n",
    "                          sort_within_batch=True, \n",
    "                          shuffle = True, \n",
    "                          repeat = False)\n",
    "# Reset variables \n",
    "accuracies = []\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "train_accuracy = 0\n",
    "step_count = 0\n",
    "max_accuracy = 0\n",
    "# Training loop\n",
    "for i in range(num_epochs):\n",
    "    print('Training epoch ',i)\n",
    "    train_iter.init_epoch()\n",
    "    for batch in train_iter:\n",
    "        x_train = batch.text\n",
    "        y_train = batch.label\n",
    "        # Forward pass\n",
    "        y_model = model(x_train)\n",
    "        # Loss function\n",
    "        loss = loss_function(y_model, y_train)\n",
    "        losses_train.append(float(loss))\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Evaluation in test set\n",
    "        if step_count%50 == 0:\n",
    "            # Calculate model in test set by pieces\n",
    "            model.eval() # Set model to eval (if there is dropout, set it to zero)\n",
    "            y_model_test_list = []\n",
    "            y_test_list = []\n",
    "            for test_batch in test_iter:\n",
    "                y_model_test_list.append(model(test_batch.text))\n",
    "                y_test_list.append(test_batch.label)\n",
    "            model.train() # Set model to train (if there is dropout, not be zero )\n",
    "            test_iter.init_epoch()\n",
    "            # Calculate accuracy\n",
    "            accuracy = float( (torch.cat(y_model_test_list).max(dim=1)[1] ==\n",
    "            torch.cat(y_test_list)).float().mean() )\n",
    "            print('Step: ', step_count, 'Accuracy in test set:', accuracy)\n",
    "            accuracies.append(accuracy)\n",
    "        lr_decay.step()\n",
    "        step_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
