{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4ded09-37c2-4efe-9140-aff5f8bbcbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: hello whats your name\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "\n",
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and normalize text\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(f\"[{punctuation}]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "input_text = \"Hello! What's your name?\"\n",
    "clean_text = preprocess_text(input_text)\n",
    "print(\"Cleaned Text:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9bfdd5-d13f-4773-9e5d-c64222a45ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Sequence: [8, 5, 12, 12, 15, 27, 23, 8, 1, 20, 19, 27, 25, 15, 21, 18, 27, 14, 1, 13, 5]\n"
     ]
    }
   ],
   "source": [
    "# Symbol-to-ID mapping (simplified)\n",
    "_symbol_to_id = {s: i for i, s in enumerate(\"abcdefghijklmnopqrstuvwxyz \", 1)}\n",
    "\n",
    "# Convert text to sequence of IDs\n",
    "def text_to_sequence(text):\n",
    "    sequence = [_symbol_to_id.get(s, 0) for s in text if s in _symbol_to_id]\n",
    "    return sequence\n",
    "\n",
    "# Example usage\n",
    "sequence = text_to_sequence(clean_text)\n",
    "print(\"Text to Sequence:\", sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e055c-6657-47c3-80c6-01ea9c34c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "\n",
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and normalize text\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(f\"[{punctuation}]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "input_text = \"Hello! What's your name?\"\n",
    "clean_text = preprocess_text(input_text)\n",
    "print(\"Cleaned Text:\", clean_text)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tacotron2 = torch.hub.load(\"nvidia/DeepLearningExamples:torchhub\", \"nvidia_tacotron2\").to(device).eval()\n",
    "\n",
    "# Function to generate mel spectrogram from text\n",
    "def generate_mel_spectrogram(text):\n",
    "    # Convert text to sequence tensor\n",
    "    sequence = torch.tensor(text_to_sequence(text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_lengths = torch.tensor([sequence.shape[1]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mel_spec, _, _ = tacotron2.infer(sequence, input_lengths)\n",
    "    return mel_spec\n",
    "\n",
    "# Generate mel spectrogram\n",
    "mel_spec = generate_mel_spectrogram(clean_text)\n",
    "print(\"Mel Spectrogram Shape:\", mel_spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d5f00-1385-4122-a032-daf5f5104af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tacotron 2 and WaveGlow models from PyTorch Hub and set them to evaluation mode\n",
    "tacotron2 = torch.hub.load(\"nvidia/DeepLearningExamples:torchhub\", \"nvidia_tacotron2\").to(device).eval()\n",
    "waveglow = torch.hub.load(\"nvidia/DeepLearningExamples:torchhub\", \"nvidia_waveglow\").to(device).eval()\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)  # Remove weight normalization for faster inference\n",
    "\n",
    "# Symbol to ID mapping for Tacotron2 text processing\n",
    "_symbol_to_id = {s: i for i, s in enumerate(\"what is your name\", 1)}\n",
    "\n",
    "# Function to clean and convert text to sequence\n",
    "def text_to_sequence(text):\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(f\"[{punctuation}]\", \"\", text)\n",
    "    sequence = [_symbol_to_id.get(s, 0) for s in text if s in _symbol_to_id]\n",
    "    return torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Define the TTS function\n",
    "def tts(text):\n",
    "    # Convert text to sequence and get input length\n",
    "    sequence = text_to_sequence(text)\n",
    "    input_lengths = torch.tensor([sequence.shape[1]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate mel spectrogram using Tacotron 2\n",
    "    with torch.no_grad():\n",
    "        # Updated line to get the first element of the tuple which is the mel spectrogram.\n",
    "        mel_spec, _, _ = tacotron2.infer(sequence, input_lengths)  \n",
    "    \n",
    "    # Generate audio waveform from mel spectrogram using WaveGlow\n",
    "    with torch.no_grad():\n",
    "        audio = waveglow.infer(mel_spec)\n",
    "    \n",
    "    return audio.cpu().numpy()\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, this is a demonstration of text-to-speech synthesis using Tacotron 2 and WaveGlow.\"\n",
    "\n",
    "# Generate audio waveform from text\n",
    "audio_waveform = tts(text)\n",
    "\n",
    "# Play audio\n",
    "ipd.Audio(audio_waveform, rate=22050)  # Sample rate for WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128ccbc-7535-435f-9165-2b6bfe41660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4af9be-9891-4dc6-ad2f-cf0a6a7739b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.speecht5.processing_speecht5 because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/speecht5/processing_speecht5.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Speech processor class for SpeechT5.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProcessorMixin\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSpeechT5Processor\u001b[39;00m(ProcessorMixin):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/processing_utils.py:33\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, is_valid_image, is_vision_available\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_utils.py:58\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     60\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST,\n\u001b[1;32m     62\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[1;32m     67\u001b[0m     }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_ops\u001b[38;5;241m.\u001b[39mimpl_abstract(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/library.py:795\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    794\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[0;32m--> 795\u001b[0m use_lib\u001b[38;5;241m.\u001b[39m_register_fake(op_name, func, _stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/library.py:184\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    182\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m--> 184\u001b[0m handle \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mfake_impl\u001b[38;5;241m.\u001b[39mregister(func_to_register, source)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_library/fake_impl.py:31\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeechT5Processor, SpeechT5ForTextToSpeech\n\u001b[1;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m SpeechT5Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/speecht5_tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SpeechT5ForTextToSpeech\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/speecht5_tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1755\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1752\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.speecht5.processing_speecht5 because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "inputs = processor(text=\"Don't count the days, make the days count.\", return_tensors=\"pt\")\n",
    "from datasets import load_dataset\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "import torch\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n",
    "from transformers import SpeechT5HifiGan\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "from IPython.display import Audio\n",
    "Audio(speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7047f92-00e2-442a-b0b8-1cda5e254a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Dataset Preparation (Example with LJSpeech)\n",
    "To load the dataset, we use torchaudio for audio files and prepare input text as a sequence of phonemes or characters.\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LJSPEECH\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = LJSPEECH(root, download=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate, transcript, *_ = self.dataset[idx]\n",
    "        \n",
    "        # Convert text to a sequence of phonemes or characters (basic example)\n",
    "        text_sequence = torch.tensor([ord(c) for c in transcript.lower() if c.isalnum()], dtype=torch.long)\n",
    "        \n",
    "        # Generate mel spectrogram\n",
    "        mel_spectrogram = transforms.MelSpectrogram()(waveform)\n",
    "        \n",
    "        return text_sequence, mel_spectrogram.squeeze(0)\n",
    "    \n",
    "# Load dataset\n",
    "dataset = TTSDataset(root=\"./data\")\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "Step 2: Define a Simplified TTS Model\n",
    "For demonstration purposes, we’ll define a basic encoder-decoder model that generates a mel spectrogram from text input. In a real TTS system, this would be more complex (e.g., Tacotron 2 with attention mechanisms).\n",
    "import torch.nn as nn\n",
    "class SimpleTTSModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, mel_bins=80):\n",
    "        super(SimpleTTSModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, mel_bins)\n",
    "\n",
    "    def forward(self, text_sequence):\n",
    "        embedded = self.embedding(text_sequence)\n",
    "        encoder_output, _ = self.encoder(embedded)\n",
    "        decoder_output, _ = self.decoder(encoder_output)\n",
    "        mel_spectrogram = self.fc(decoder_output)\n",
    "        return mel_spectrogram\n",
    "\n",
    "\n",
    "Step 3: Define Loss Functions\n",
    "The model uses a spectrogram loss (e.g., mean squared error) to compare the generated and target mel spectrograms. Optionally, you can add a stop token loss for more complex TTS systems.\n",
    "import torch.nn.functional as F\n",
    "def spectrogram_loss(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "def stop_token_loss(pred_stop, target_stop):\n",
    "    return F.binary_cross_entropy_with_logits(pred_stop, target_stop)\n",
    "Step 4: Training Loop\n",
    "Now we’ll set up the training loop, including the optimizer and gradient clipping.\n",
    "# Model parameters\n",
    "vocab_size = 128  # ASCII characters used in the text sequence\n",
    "model = SimpleTTSModel(vocab_size).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "   \n",
    "    for text_sequence, target_mel in dataloader:\n",
    "        # Move to device\n",
    "        text_sequence = text_sequence.to(device)\n",
    "        target_mel = target_mel.to(device)\n",
    "        # Forward pass\n",
    "        predicted_mel = model(text_sequence)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = spectrogram_loss(predicted_mel, target_mel)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
